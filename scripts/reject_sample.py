#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Reject Sampling é€»è¾‘ï¼š
1. åŠ è½½ HuggingFace æ•°æ®é›† Coobiw/merged_agiqa5k_prompt_1022 (train split)
2. å¯åŠ¨ vLLM æœåŠ¡
3. å¯¹æ¯æ¡æ ·æœ¬é€šè¿‡ vLLM æœåŠ¡ rollout N æ¬¡ï¼ˆé»˜è®¤16æ¬¡ï¼‰
4. è®¡ç®—æ¯æ¬¡è¾“å‡ºå’Œ ground truth answer çš„å·®å€¼
5. å–æœ€å°å·®å€¼ä½œä¸ºè¯¥æ ·æœ¬çš„è¯¯å·®
6. ç”»ç›´æ–¹å›¾ç»Ÿè®¡è¯¯å·®åˆ†å¸ƒ
7. åœæ­¢ vLLM æœåŠ¡
"""

import os
import io
import argparse
import json
import time
import signal
import subprocess
import requests
from pathlib import Path
from tqdm import tqdm
import base64
import asyncio
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

from openai import AsyncOpenAI
from datasets import load_dataset

# ----------- vLLM æœåŠ¡ç®¡ç† ----------------------------------------------------
def wait_for_server(port=8000, timeout=600, check_interval=10):
    """ç­‰å¾… vLLM æœåŠ¡å¯åŠ¨"""
    print(f"Waiting for vLLM server to start on port {port}...")
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            response = requests.get(f"http://localhost:{port}/health", timeout=5)
            if response.status_code == 200:
                print("âœ… vLLM server is ready!")
                return True
        except Exception:
            pass
        time.sleep(check_interval)
    raise TimeoutError(f"vLLM server did not start within {timeout} seconds")

def kill_process_group(process):
    """å®‰å…¨åœ° kill è¿›ç¨‹ç»„"""
    try:
        # å‘é€ SIGTERM åˆ°æ•´ä¸ªè¿›ç¨‹ç»„
        os.killpg(os.getpgid(process.pid), signal.SIGTERM)
        # ç­‰å¾…è¿›ç¨‹ç»“æŸï¼Œæœ€å¤šç­‰å¾…10ç§’
        process.wait(timeout=10)
    except subprocess.TimeoutExpired:
        # å¦‚æœè¿›ç¨‹è¿˜æ²¡ç»“æŸï¼Œå¼ºåˆ¶ kill
        print("Force killing vLLM server with SIGKILL...")
        try:
            os.killpg(os.getpgid(process.pid), signal.SIGKILL)
            process.wait()
        except:
            pass
    except (ProcessLookupError, PermissionError):
        # è¿›ç¨‹å·²ç»ä¸å­˜åœ¨äº†
        pass

# ----------- å·¥å…·å‡½æ•° ---------------------------------------------------------
def pil2base64(image):
    """å°† PIL Image è½¬æ¢ä¸º base64 ç¼–ç """
    with io.BytesIO() as output:
        image.save(output, format="PNG")
        return base64.b64encode(output.getvalue()).decode("utf-8")

def extract_answer_value(response_text: str) -> float:
    """ä»æ¨¡å‹å“åº”ä¸­æå–ç­”æ¡ˆæ•°å€¼"""
    try:
        # å°è¯•æå– <answer> æ ‡ç­¾ä¸­çš„å†…å®¹
        answer_start = response_text.find("<answer>")
        answer_end = response_text.find("</answer>", answer_start + len("<answer>"))
        
        if answer_end == -1:
            if answer_start == -1:
                # æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œç›´æ¥ç”¨æ•´ä¸ªå“åº”
                answer_text = response_text
            else:
                # åªæ‰¾åˆ°å¼€å§‹æ ‡ç­¾
                answer_text = response_text[answer_start + len("<answer>"):]
        else:
            # æ‰¾åˆ°å®Œæ•´æ ‡ç­¾å¯¹
            answer_text = response_text[answer_start + len("<answer>"):answer_end]
        
        # è½¬æ¢ä¸ºæµ®ç‚¹æ•°
        value = float(answer_text.strip())
        return value
    except Exception as e:
        # è§£æå¤±è´¥è¿”å› None
        return None

def compute_error(pred_value: float, gt_value: float) -> float:
    """è®¡ç®—é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„å·®å€¼ï¼ˆç»å¯¹è¯¯å·®ï¼‰"""
    if pred_value is None:
        return float('inf')  # è§£æå¤±è´¥è§†ä¸ºæ— ç©·å¤§è¯¯å·®
    return abs(pred_value - gt_value)

# ----------- æ•°æ®ç”Ÿæˆå™¨ -------------------------------------------------------
def prepare_dataset(dataset, sys_prompt: str):
    """å‡†å¤‡æ•°æ®é›†ï¼Œè½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼"""
    prepared_data = []
    for data_item in tqdm(dataset, desc="Preparing Dataset: "):
        # æ£€æŸ¥æ•°æ®é¡¹ç»“æ„
        if 'images' in data_item and len(data_item['images']) > 0:
            image = data_item['images'][0]  # PIL.Image
            query = f"{data_item['problem'].replace('<image>', '').strip()} {sys_prompt}"
            gt_answer = data_item['answer']
            
            prepared_data.append({
                "messages": [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image;base64,{pil2base64(image)}"}
                            },
                            {"type": "text", "text": query}
                        ],
                    }
                ],
                "ground_truth": gt_answer,
                "problem": data_item['problem'],
            })
    return prepared_data

# ----------- Async æ¨ç†ï¼ˆæ”¯æŒå¤šæ¬¡é‡‡æ ·ï¼‰----------------------------------------
async def async_single_rollout(
    client: AsyncOpenAI,
    item: dict,
    model_name: str,
    temperature: float = 1.0,
    max_retries: int = 10,
):
    """å•æ¬¡å¼‚æ­¥æ¨ç†ï¼ˆå¸¦é‡è¯•é€»è¾‘ï¼‰"""
    for retry in range(max_retries):
        try:
            resp = await client.chat.completions.create(
                model=model_name,
                messages=item["messages"],
                temperature=temperature,
                max_tokens=4096,
            )
            ret = resp.choices[0].message.content
            print(f"\033[91m{ret}\033[0m")
            return ret
        except Exception as e:
            if retry < max_retries - 1:
                wait_time = min(2 ** retry, 30)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤šç­‰å¾…30ç§’
                # åªåœ¨å‰3æ¬¡é‡è¯•æ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯ï¼Œé¿å…åˆ·å±
                if retry < 3:
                    tqdm.write(f"âš ï¸  Retry {retry+1}/{max_retries}: {str(e)[:50]}... (wait {wait_time}s)")
                await asyncio.sleep(wait_time)
            else:
                tqdm.write(f"âŒ Failed after {max_retries} retries: {str(e)[:80]}")
                return None

async def async_reject_sample_item(
    client: AsyncOpenAI,
    item: dict,
    model_name: str,
    num_rollout: int = 16,
    temperature: float = 0.8,
    max_retries: int = 10,
):
    """å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œ reject samplingï¼ˆå¤šæ¬¡ rolloutï¼‰- ä¸å†ä½¿ç”¨ semaphore"""
    return await _async_reject_sample_item_impl(
        client, item, model_name, num_rollout, temperature, max_retries
    )

async def _async_reject_sample_item_impl(
    client: AsyncOpenAI,
    item: dict,
    model_name: str,
    num_rollout: int,
    temperature: float,
    max_retries: int,
):
    """å®é™…çš„ reject sampling å®ç°"""
    gt_value = float(item["ground_truth"])
    
    # å¹¶å‘è¿›è¡Œ num_rollout æ¬¡ rollout
    tasks = [
        asyncio.create_task(
            async_single_rollout(client, item, model_name, temperature, max_retries)
        )
        for _ in range(num_rollout)
    ]
    responses = await asyncio.gather(*tasks)
    
    # è®¡ç®—æ¯æ¬¡çš„è¯¯å·®
    errors = []
    valid_responses = []
    for resp in responses:
        if resp is not None:
            pred_value = extract_answer_value(resp)
            error = compute_error(pred_value, gt_value)
            errors.append(error)
            valid_responses.append({
                "response": resp,
                "pred_value": pred_value,
                "error": error,
            })
    
    # æ‰¾åˆ°æœ€å°è¯¯å·®
    if len(errors) > 0:
        min_error = min(errors)
        min_error_idx = errors.index(min_error)
        best_response = valid_responses[min_error_idx]
    else:
        min_error = float('inf')
        best_response = None
    
    return {
        "ground_truth": gt_value,
        "min_error": min_error,
        "num_valid_rollouts": len(errors),
        "num_total_rollouts": num_rollout,
        "best_response": best_response,
        "all_errors": errors,
    }

# ----------- ä¸»å…¥å£ -----------------------------------------------------------
async def main():
    parser = argparse.ArgumentParser(description="Reject Sampling for AGIQA Dataset")
    parser.add_argument("--model_path", type=str, required=True, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--model_name", type=str, required=True, help="æ¨¡å‹åç§°")
    parser.add_argument("--model_port", type=int, default=8000, help="vLLM Server ç«¯å£")
    parser.add_argument("--num_rollout", type=int, default=16, help="æ¯æ¡æ•°æ® rollout æ¬¡æ•°")
    parser.add_argument("--temperature", type=float, default=1.0, help="é‡‡æ ·æ¸©åº¦")
    parser.add_argument("--concurrency", type=int, default=8, help="å¹¶å‘å¤„ç†çš„æ•°æ®æ¡æ•°")
    parser.add_argument("--max_retries", type=int, default=10, help="rollout å¤±è´¥æ—¶çš„é‡è¯•æ¬¡æ•°")
    parser.add_argument("--max_data", type=int, default=None, help="æœ€å¤§å¤„ç†æ•°æ®é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    parser.add_argument("--output_dir", type=str, default="./reject_sampling_results", 
                        help="ç»“æœä¿å­˜ç›®å½•")
    parser.add_argument("--log_dir", type=str, default="./logs", help="vLLM æœåŠ¡æ—¥å¿—ç›®å½•")
    parser.add_argument("--skip_vllm_launch", action="store_true", 
                        help="è·³è¿‡ vLLM æœåŠ¡å¯åŠ¨ï¼ˆå‡è®¾æœåŠ¡å·²åœ¨è¿è¡Œï¼‰")
    args = parser.parse_args()
    
    print("="*80)
    print("Reject Sampling with vLLM")
    print("="*80)
    print(f"Model Path: {args.model_path}")
    print(f"Model Name: {args.model_name}")
    print(f"Model Port: {args.model_port}")
    print(f"Rollouts per sample: {args.num_rollout}")
    print(f"Temperature: {args.temperature}")
    print(f"Max Retries: {args.max_retries}")
    print(f"Concurrency: {args.concurrency}")
    print("="*80)
    print()
    
    # ç³»ç»Ÿæç¤ºè¯
    sys_prompt = (
        "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. "
        "The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. "
        "The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, "
        "i.e., <think> reasoning process here </think><answer> answer here </answer>"
    )
    
    # å¯åŠ¨ vLLM æœåŠ¡
    serve_process = None
    if not args.skip_vllm_launch:
        log_dir = Path(args.log_dir)
        log_dir.mkdir(parents=True, exist_ok=True)
        log_file = log_dir / f"{args.model_name}_vllm.log"
        
        print(f"Starting vLLM server for {args.model_name}...")
        print(f"Server logs will be saved to: {log_file}")
        
        with open(log_file, "w") as f:
            serve_process = subprocess.Popen(
                ["bash", "scripts/vllm_serve.sh", args.model_path, args.model_name],
                stdout=f,
                stderr=subprocess.STDOUT,
                preexec_fn=os.setsid  # åˆ›å»ºæ–°çš„è¿›ç¨‹ç»„
            )
        
        try:
            # ç­‰å¾…æœåŠ¡å¯åŠ¨
            wait_for_server(port=args.model_port, timeout=600)
            
            # é¢å¤–ç­‰å¾…ç¡®ä¿æ¨¡å‹åŠ è½½å®Œæˆ
            print("Waiting additional 30 seconds for model to fully load...")
            time.sleep(30)
        except TimeoutError as e:
            print(f"âŒ Timeout error: {e}")
            print(f"   Check vLLM logs at: {log_file}")
            if serve_process:
                kill_process_group(serve_process)
            return
        except Exception as e:
            print(f"âŒ Error starting vLLM server: {e}")
            if serve_process:
                kill_process_group(serve_process)
            return
    else:
        print("âš ï¸  Skipping vLLM launch, assuming service is already running...")
    
    try:
        # åŠ è½½æ•°æ®é›†
        print("\nLoading dataset from HuggingFace...")
        dataset = load_dataset("Coobiw/merged_agiqa5k_prompt_1022", split="train")
        print(f"Dataset loaded: {len(dataset)} samples")
        
        # å‡†å¤‡æ•°æ®
        print("Preparing data...")
        prepared_data = prepare_dataset(dataset, sys_prompt)
        del dataset  # é‡Šæ”¾å†…å­˜
        
        # é™åˆ¶æ•°æ®é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if args.max_data is not None:
            prepared_data = prepared_data[:args.max_data]
            print(f"Limited to {len(prepared_data)} samples for testing")
        
        # è®¾ç½® OpenAI å®¢æˆ·ç«¯
        openai_api_base = f"http://localhost:{args.model_port}/v1"
        client = AsyncOpenAI(api_key="EMPTY", base_url=openai_api_base)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(args.output_dir) / args.model_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¿›è¡Œ reject sampling
        print(f"\nStarting reject sampling with {args.num_rollout} rollouts per data point...")
        print(f"Sample-level Concurrency: {args.concurrency} samples in parallel")
        print(f"Rollout-level Concurrency: {args.num_rollout} rollouts per sample")
        print(f"Total parallel requests: up to {args.concurrency * args.num_rollout}")
        print(f"Temperature: {args.temperature}")
        print(f"Max Retries: {args.max_retries}")
        print()
        print(f"â³ Note: First batch may take longer (model warmup)...")
        print(f"ğŸ“Š Progress bar updates every time a sample completes (not just per batch)")
        print()
        
        results = []
        total_rollouts = 0
        successful_rollouts = 0
        
        # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹å¹¶å‘å¤„ç† concurrency ä¸ªæ ·æœ¬
        batch_size = args.concurrency
        num_batches = (len(prepared_data) + batch_size - 1) // batch_size
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºæ ·æœ¬çº§åˆ«çš„è¿›åº¦
        with tqdm(total=len(prepared_data), desc="Processing Samples", unit="sample", 
                  smoothing=0.1, mininterval=0.5) as pbar:
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(prepared_data))
                batch = prepared_data[start_idx:end_idx]
                
                # å¹¶å‘å¤„ç†ä¸€ä¸ª batch å†…çš„æ‰€æœ‰æ ·æœ¬
                # åˆ›å»ºä»»åŠ¡å­—å…¸ï¼Œç”¨äºè¿½è¸ª task å’Œ item çš„å¯¹åº”å…³ç³»
                task_to_item = {}
                for item in batch:
                    task = asyncio.create_task(
                        async_reject_sample_item(
                            client=client,
                            item=item,
                            model_name=args.model_name,
                            num_rollout=args.num_rollout,
                            temperature=args.temperature,
                            max_retries=args.max_retries,
                        )
                    )
                    task_to_item[task] = item
                
                # ä½¿ç”¨ wait æ¥å®æ—¶ç›‘æ§ä»»åŠ¡å®Œæˆæƒ…å†µ
                pending = set(task_to_item.keys())
                while pending:
                    # ç­‰å¾…è‡³å°‘ä¸€ä¸ªä»»åŠ¡å®Œæˆ
                    done, pending = await asyncio.wait(pending, return_when=asyncio.FIRST_COMPLETED)
                    
                    # å¤„ç†æ‰€æœ‰å·²å®Œæˆçš„ä»»åŠ¡
                    for task in done:
                        result = await task
                        item = task_to_item[task]
                        result["problem"] = item["problem"]
                        results.append(result)
                        
                        # æ›´æ–°ç»Ÿè®¡
                        total_rollouts += result["num_total_rollouts"]
                        successful_rollouts += result["num_valid_rollouts"]
                        success_rate = (successful_rollouts / total_rollouts * 100) if total_rollouts > 0 else 0
                        avg_valid = successful_rollouts / len(results) if results else 0
                        
                        # è®¡ç®—æœ€è¿‘çš„å¹³å‡è¯¯å·®ï¼ˆé¿å…é‡å¤è®¡ç®—æ‰€æœ‰ç»“æœï¼‰
                        recent_errors = [r["min_error"] for r in results[-100:] if r["min_error"] != float('inf')]
                        avg_min_err = np.mean(recent_errors) if recent_errors else 0
                        
                        # å®æ—¶æ›´æ–°è¿›åº¦æ¡
                        pbar.set_postfix({
                            'batch': f'{batch_idx+1}/{num_batches}',
                            'valid': f'{result["num_valid_rollouts"]}/{args.num_rollout}',
                            'err': f'{result["min_error"]:.3f}' if result["min_error"] != float('inf') else 'inf',
                            'avg': f'{avg_min_err:.3f}',
                            'succ': f'{success_rate:.1f}%'
                        })
                        pbar.update(1)
    
        # æ‰“å° rollout ç»Ÿè®¡
        print(f"\n{'='*80}")
        print("ROLLOUT STATISTICS")
        print(f"{'='*80}")
        print(f"Total samples processed: {len(results)}")
        print(f"Total rollouts attempted: {total_rollouts}")
        print(f"Successful rollouts: {successful_rollouts}")
        print(f"Failed rollouts: {total_rollouts - successful_rollouts}")
        print(f"Overall success rate: {success_rate:.2f}%")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        output_file = output_dir / "reject_sampling_results.json"
        with open(output_file, "w") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… Detailed results saved to: {output_file}")
        
        # ç»Ÿè®¡åˆ†æ
        print("\n" + "="*80)
        print("STATISTICS")
        print("="*80)
        
        min_errors = [r["min_error"] for r in results if r["min_error"] != float('inf')]
        valid_count = len(min_errors)
        total_count = len(results)
        
        print(f"\nTotal samples: {total_count}")
        print(f"Valid samples: {valid_count} ({valid_count/total_count*100:.2f}%)")
        print(f"Failed samples: {total_count - valid_count}")
        
        if len(min_errors) > 0:
            print(f"\nError Statistics:")
            print(f"  Mean error: {np.mean(min_errors):.4f}")
            print(f"  Median error: {np.median(min_errors):.4f}")
            print(f"  Std error: {np.std(min_errors):.4f}")
            print(f"  Min error: {np.min(min_errors):.4f}")
            print(f"  Max error: {np.max(min_errors):.4f}")
            
            # è¯¯å·®åˆ†å¸ƒç»Ÿè®¡
            print(f"\nError Distribution:")
            error_bins = [0, 0.1, 0.2, 0.3, 0.5, 0.8, 1.0, 1.5, 2.0, float('inf')]
            error_counts = defaultdict(int)
            for err in min_errors:
                for i in range(len(error_bins) - 1):
                    if error_bins[i] <= err < error_bins[i+1]:
                        bin_name = f"[{error_bins[i]}, {error_bins[i+1]})"
                        error_counts[bin_name] += 1
                        break
            
            for bin_name, count in sorted(error_counts.items()):
                percentage = count / len(min_errors) * 100
                print(f"  {bin_name}: {count} ({percentage:.2f}%)")
            
            # ç”»ç›´æ–¹å›¾
            print(f"\nGenerating histogram...")
            plt.figure(figsize=(12, 6))
            
            # å­å›¾1ï¼šæ‰€æœ‰è¯¯å·®
            plt.subplot(1, 2, 1)
            plt.hist(min_errors, bins=50, edgecolor='black', alpha=0.7)
            plt.xlabel('Minimum Error')
            plt.ylabel('Frequency')
            plt.title(f'Distribution of Minimum Errors\n(n={len(min_errors)})')
            plt.grid(True, alpha=0.3)
            
            # å­å›¾2ï¼šè¯¯å·® < 2.0 çš„éƒ¨åˆ†ï¼ˆæ”¾å¤§çœ‹ç»†èŠ‚ï¼‰
            filtered_errors = [e for e in min_errors if e < 2.0]
            plt.subplot(1, 2, 2)
            plt.hist(filtered_errors, bins=50, edgecolor='black', alpha=0.7, color='orange')
            plt.xlabel('Minimum Error')
            plt.ylabel('Frequency')
            plt.title(f'Distribution of Minimum Errors (< 2.0)\n(n={len(filtered_errors)})')
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            hist_file = output_dir / "error_distribution.png"
            plt.savefig(hist_file, dpi=300, bbox_inches='tight')
            print(f"âœ… Histogram saved to: {hist_file}")
            
            # ä¿å­˜ç»Ÿè®¡æ‘˜è¦
            stats_file = output_dir / "statistics.txt"
            with open(stats_file, "w") as f:
                f.write(f"Reject Sampling Statistics\n")
                f.write(f"="*80 + "\n\n")
                f.write(f"Model: {args.model_name}\n")
                f.write(f"Rollouts per data point: {args.num_rollout}\n")
                f.write(f"Temperature: {args.temperature}\n")
                f.write(f"Max Retries: {args.max_retries}\n\n")
                f.write(f"Total samples: {total_count}\n")
                f.write(f"Valid samples: {valid_count} ({valid_count/total_count*100:.2f}%)\n")
                f.write(f"Failed samples: {total_count - valid_count}\n\n")
                f.write(f"Error Statistics:\n")
                f.write(f"  Mean: {np.mean(min_errors):.4f}\n")
                f.write(f"  Median: {np.median(min_errors):.4f}\n")
                f.write(f"  Std: {np.std(min_errors):.4f}\n")
                f.write(f"  Min: {np.min(min_errors):.4f}\n")
                f.write(f"  Max: {np.max(min_errors):.4f}\n\n")
                f.write(f"Error Distribution:\n")
                for bin_name, count in sorted(error_counts.items()):
                    percentage = count / len(min_errors) * 100
                    f.write(f"  {bin_name}: {count} ({percentage:.2f}%)\n")
            print(f"âœ… Statistics saved to: {stats_file}")
        else:
            print("\nâŒ No valid samples to analyze!")
        
        print("\n" + "="*80)
        print("COMPLETED")
        print("="*80)
        
    finally:
        # åœæ­¢ vLLM æœåŠ¡
        if serve_process is not None:
            print("\nStopping vLLM server...")
            kill_process_group(serve_process)
            print("vLLM server stopped")
            
            # ç­‰å¾…ç«¯å£é‡Šæ”¾
            print("Waiting for port to be released...")
            time.sleep(10)

if __name__ == "__main__":
    asyncio.run(main())


