# compute_new_adv ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

`compute_new_adv` æ˜¯ GRPO çš„ä¸€ä¸ªæ–°åŠŸèƒ½ï¼Œç”¨äºåœ¨æ ·æœ¬è¿‡æ»¤åé‡æ–°è®¡ç®— advantagesã€‚å½“ä½¿ç”¨ `keep_pos_ratio` å’Œ `keep_neg_ratio` è¿‡æ»¤æ ·æœ¬åï¼Œå¯ä»¥é€‰æ‹©ç”¨ä¿ç•™çš„æ ·æœ¬é‡æ–°è®¡ç®—å½’ä¸€åŒ–çš„ advantagesï¼Œè€Œä¸æ˜¯ç›´æ¥ä½¿ç”¨åŸå§‹çš„ advantagesã€‚

## åŠŸèƒ½è¯´æ˜

### æ ‡å‡† GRPO æµç¨‹
1. è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„ GRPO advantagesï¼ˆä½¿ç”¨æ‰€æœ‰æ ·æœ¬çš„å‡å€¼å’Œæ ‡å‡†å·®å½’ä¸€åŒ–ï¼‰
2. æ ¹æ® `keep_pos_ratio` å’Œ `keep_neg_ratio` è¿‡æ»¤æ ·æœ¬
3. ä½¿ç”¨åŸå§‹ advantages è¿›è¡Œè®­ç»ƒ

### å¯ç”¨ compute_new_adv åçš„æµç¨‹
1. è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„ GRPO advantagesï¼ˆä½¿ç”¨æ‰€æœ‰æ ·æœ¬çš„å‡å€¼å’Œæ ‡å‡†å·®å½’ä¸€åŒ–ï¼‰
2. æ ¹æ® `keep_pos_ratio` å’Œ `keep_neg_ratio` è¿‡æ»¤æ ·æœ¬
3. **é‡æ–°è®¡ç®—ä¿ç•™æ ·æœ¬çš„ advantages**ï¼ˆä½¿ç”¨ä¿ç•™æ ·æœ¬çš„å‡å€¼å’Œæ ‡å‡†å·®é‡æ–°å½’ä¸€åŒ–ï¼‰
4. ä½¿ç”¨é‡æ–°è®¡ç®—çš„ advantages è¿›è¡Œè®­ç»ƒ

## ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªåŠŸèƒ½ï¼Ÿ

### åŠ¨æœº
å½“æˆ‘ä»¬è¿‡æ»¤æ‰ä¸€äº›æ ·æœ¬åï¼Œä¿ç•™æ ·æœ¬çš„åˆ†å¸ƒå¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚ä¾‹å¦‚ï¼š

- **åŸå§‹æ ·æœ¬**ï¼šrewards = [0.1, 0.5, 0.9, 1.3, 1.7]
  - mean = 0.9, std = 0.6
  - advantages = [-1.33, -0.67, 0, 0.67, 1.33]

- **è¿‡æ»¤å**ï¼ˆå‡è®¾åªä¿ç•™æœ€å¥½çš„3ä¸ªï¼‰ï¼šrewards = [0.9, 1.3, 1.7]
  - ä½¿ç”¨åŸå§‹ advantagesï¼š[0, 0.67, 1.33]
  - ä½¿ç”¨é‡æ–°è®¡ç®—çš„ advantagesï¼š[-1, 0, 1]ï¼ˆmean=1.3, std=0.4ï¼‰

### ä¼˜åŠ¿
1. **æ›´å¼ºçš„è®­ç»ƒä¿¡å·**ï¼šä¿ç•™æ ·æœ¬ä¹‹é—´çš„ advantage å·®å¼‚æ›´æ˜æ˜¾
2. **æ›´å¥½çš„å½’ä¸€åŒ–**ï¼šadvantages æ›´ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒ
3. **æ›´é›†ä¸­çš„å­¦ä¹ **ï¼šæ¨¡å‹æ›´ä¸“æ³¨äºä¿ç•™æ ·æœ¬ä¹‹é—´çš„ç›¸å¯¹å·®å¼‚

## é…ç½®å‚æ•°

åœ¨ `algorithm` é…ç½®ä¸­æ·»åŠ ï¼š

```yaml
algorithm:
  adv_estimator: grpo
  keep_neg_ratio: 0.5      # ä¿ç•™æœ€å·®çš„ 50% è´Ÿæ ·æœ¬
  keep_pos_ratio: 0.7      # ä¿ç•™æœ€å¥½çš„ 70% æ­£æ ·æœ¬
  compute_new_adv: true    # å¯ç”¨é‡æ–°è®¡ç®— advantages
```

### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `compute_new_adv` | bool | `false` | æ˜¯å¦ç”¨ä¿ç•™æ ·æœ¬é‡æ–°è®¡ç®— advantages |
| `keep_pos_ratio` | float | `1.0` | æ­£æ ·æœ¬ä¿ç•™æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰ |
| `keep_neg_ratio` | float | `1.0` | è´Ÿæ ·æœ¬ä¿ç•™æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰ |

## ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šåŸºç¡€ç”¨æ³•
```yaml
algorithm:
  adv_estimator: grpo
  keep_neg_ratio: 0.5
  keep_pos_ratio: 0.7
  compute_new_adv: true
```

è¿™ä¼šï¼š
1. ä¿ç•™æœ€å·®çš„ 50% è´Ÿæ ·æœ¬ï¼ˆadvantage æœ€ä½ï¼‰
2. ä¿ç•™æœ€å¥½çš„ 70% æ­£æ ·æœ¬ï¼ˆadvantage æœ€é«˜ï¼‰
3. ç”¨è¿™äº›ä¿ç•™çš„æ ·æœ¬é‡æ–°è®¡ç®— advantages

### ç¤ºä¾‹ 2ï¼šå‘½ä»¤è¡Œé…ç½®
```bash
python3 -m verl.trainer.main \
    config=examples/config_agiqa3k.yaml \
    algorithm.keep_pos_ratio=0.7 \
    algorithm.keep_neg_ratio=0.5 \
    algorithm.compute_new_adv=true
```

### ç¤ºä¾‹ 3ï¼šåªè¿‡æ»¤è´Ÿæ ·æœ¬
```yaml
algorithm:
  adv_estimator: grpo
  keep_neg_ratio: 0.3      # åªä¿ç•™æœ€å·®çš„ 30% è´Ÿæ ·æœ¬
  keep_pos_ratio: 1.0      # ä¿ç•™æ‰€æœ‰æ­£æ ·æœ¬
  compute_new_adv: true    # é‡æ–°è®¡ç®—
```

### ç¤ºä¾‹ 4ï¼šåªè¿‡æ»¤æ­£æ ·æœ¬
```yaml
algorithm:
  adv_estimator: grpo
  keep_neg_ratio: 1.0      # ä¿ç•™æ‰€æœ‰è´Ÿæ ·æœ¬
  keep_pos_ratio: 0.5      # åªä¿ç•™æœ€å¥½çš„ 50% æ­£æ ·æœ¬
  compute_new_adv: true    # é‡æ–°è®¡ç®—
```

## æ³¨æ„äº‹é¡¹

### 1. ä»…åœ¨æœ‰è¿‡æ»¤æ—¶æœ‰æ•ˆ
å¦‚æœ `keep_neg_ratio=1.0` ä¸” `keep_pos_ratio=1.0`ï¼ˆæ²¡æœ‰è¿‡æ»¤ä»»ä½•æ ·æœ¬ï¼‰ï¼Œè®¾ç½® `compute_new_adv=true` ä¸ä¼šæœ‰ä»»ä½•æ•ˆæœï¼Œç³»ç»Ÿä¼šç»™å‡ºè­¦å‘Šã€‚

### 2. æœ€å°æ ·æœ¬æ•°è¦æ±‚
æ¯ä¸ª group ä¸­è‡³å°‘éœ€è¦ä¿ç•™ 2 ä¸ªæ ·æœ¬æ‰èƒ½é‡æ–°è®¡ç®—æ ‡å‡†å·®ã€‚å¦‚æœä¿ç•™æ ·æœ¬å°‘äº 2 ä¸ªï¼Œä¼šä½¿ç”¨åŸå§‹ advantagesã€‚

### 3. ç®—æ³•é™åˆ¶
`compute_new_adv` ä»…æ”¯æŒ GRPO ç®—æ³•ï¼Œä¸æ”¯æŒå…¶ä»–ç®—æ³•ï¼ˆGAE, RLOO, REINFORCE++, REMAXï¼‰ã€‚

### 4. æ€§èƒ½è€ƒè™‘
é‡æ–°è®¡ç®— advantages ä¼šå¢åŠ å°‘é‡è®¡ç®—å¼€é”€ï¼ˆé€šå¸¸å¯å¿½ç•¥ï¼‰ï¼Œä½†å¯èƒ½å¸¦æ¥æ›´å¥½çš„è®­ç»ƒæ•ˆæœã€‚

## å®ç°ç»†èŠ‚

### é‡æ–°è®¡ç®—é€»è¾‘

å¯¹äºæ¯ä¸ª groupï¼ˆåŒä¸€ä¸ª prompt çš„å¤šä¸ª responsesï¼‰ï¼š

1. **æ‰¾å‡ºä¿ç•™çš„æ ·æœ¬**ï¼šæ ¹æ® `keep_pos_ratio` å’Œ `keep_neg_ratio` ç¡®å®š
2. **é‡æ–°è®¡ç®—ç»Ÿè®¡é‡**ï¼š
   ```python
   kept_scores = [score for kept samples]
   kept_mean = mean(kept_scores)
   kept_std = std(kept_scores)
   ```
3. **é‡æ–°å½’ä¸€åŒ–**ï¼š
   ```python
   new_advantage = (score - kept_mean) / kept_std
   ```

### ç‰¹æ®Šæƒ…å†µå¤„ç†

- **æ‰€æœ‰ä¿ç•™æ ·æœ¬å¾—åˆ†ç›¸åŒ**ï¼ˆstd â‰ˆ 0ï¼‰ï¼šadvantage è®¾ä¸º 0
- **ä¿ç•™æ ·æœ¬å°‘äº 2 ä¸ª**ï¼šä½¿ç”¨åŸå§‹ advantagesï¼ˆä¸é‡æ–°è®¡ç®—ï¼‰
- **æ— è¿‡æ»¤**ï¼ˆkeep_pos_ratio=1.0 ä¸” keep_neg_ratio=1.0ï¼‰ï¼šç›´æ¥è¿”å›åŸå§‹ advantages

## Metrics å˜åŒ–

å¯ç”¨ `compute_new_adv` åï¼Œmetrics ä¸­çš„å«ä¹‰ï¼š

| Metric | å«ä¹‰ |
|--------|------|
| `critic/advantages/*` | åŸå§‹ GRPO advantagesï¼ˆç”¨äºä¸å…¶ä»–å®éªŒå¯¹æ¯”ï¼‰ |
| `critic/advantages_processed/*` | å®é™…ç”¨äºè®­ç»ƒçš„ advantagesï¼ˆé‡æ–°è®¡ç®—åçš„ï¼‰ |
| `critic/advantages_processed/std` | ä¿ç•™æ ·æœ¬çš„ advantage æ ‡å‡†å·® |

æ³¨æ„ï¼š
- `critic/advantages_processed/*` ç»Ÿè®¡æ—¶ä¼šæ’é™¤è¢«è¿‡æ»¤æ‰çš„æ ·æœ¬ï¼ˆadvantage=0ï¼‰
- å¦‚æœ `compute_new_adv=false`ï¼Œ`advantages_processed` å°±æ˜¯åŸå§‹ advantagesï¼ˆä½†å·²è¿‡æ»¤ï¼‰

## æ¨èè®¾ç½®

### ä¿å®ˆè®¾ç½®ï¼ˆæ¨èåˆæ¬¡ä½¿ç”¨ï¼‰
```yaml
algorithm:
  keep_neg_ratio: 0.7
  keep_pos_ratio: 0.8
  compute_new_adv: true
```

### æ¿€è¿›è®¾ç½®ï¼ˆæ›´å¼ºçš„æ ·æœ¬é€‰æ‹©ï¼‰
```yaml
algorithm:
  keep_neg_ratio: 0.3
  keep_pos_ratio: 0.5
  compute_new_adv: true
```

### å¯¹æ¯”å®éªŒ
å»ºè®®è¿›è¡Œ A/B æµ‹è¯•ï¼š
- **Baseline**: `compute_new_adv: false`ï¼ˆä½¿ç”¨åŸå§‹ advantagesï¼‰
- **Treatment**: `compute_new_adv: true`ï¼ˆé‡æ–°è®¡ç®— advantagesï¼‰

## ä¸å…¶ä»–åŠŸèƒ½çš„ç»„åˆ

### ä¸ KL æƒ©ç½šç»“åˆ
```yaml
algorithm:
  adv_estimator: grpo
  keep_neg_ratio: 0.5
  keep_pos_ratio: 0.7
  compute_new_adv: true
  disable_kl: false
  kl_coef: 1.0e-2
```

### ä¸ DAPO ç»“åˆ
```yaml
algorithm:
  adv_estimator: grpo
  keep_neg_ratio: 0.5
  keep_pos_ratio: 0.7
  compute_new_adv: true

worker:
  actor:
    clip_ratio_low: 0.2
    clip_ratio_high: 0.28
    clip_ratio_dual: 10.0
```

## FAQ

### Q1: compute_new_adv ä¼šå½±å“æ”¶æ•›é€Ÿåº¦å—ï¼Ÿ
A: å¯èƒ½ä¼šã€‚é‡æ–°è®¡ç®—åçš„ advantages é€šå¸¸æ›´æç«¯ï¼ˆæ–¹å·®æ›´å¤§ï¼‰ï¼Œå¯èƒ½åŠ å¿«æ”¶æ•›ï¼Œä½†ä¹Ÿå¯èƒ½å¯¼è‡´ä¸ç¨³å®šã€‚å»ºè®®ç»“åˆè¾ƒå°çš„å­¦ä¹ ç‡ä½¿ç”¨ã€‚

### Q2: åº”è¯¥å…ˆè°ƒæ•´è¿‡æ»¤æ¯”ä¾‹è¿˜æ˜¯å…ˆå¯ç”¨ compute_new_advï¼Ÿ
A: å»ºè®®å…ˆè°ƒæ•´è¿‡æ»¤æ¯”ä¾‹ï¼ˆ`keep_pos_ratio` å’Œ `keep_neg_ratio`ï¼‰ï¼Œæ‰¾åˆ°åˆé€‚çš„è¿‡æ»¤ç­–ç•¥åï¼Œå†å°è¯•å¯ç”¨ `compute_new_adv`ã€‚

### Q3: å¯ä»¥åªå¯ç”¨ compute_new_adv è€Œä¸è¿‡æ»¤æ ·æœ¬å—ï¼Ÿ
A: æŠ€æœ¯ä¸Šå¯ä»¥ï¼Œä½†æ²¡æœ‰æ„ä¹‰ã€‚å¦‚æœä¸è¿‡æ»¤æ ·æœ¬ï¼Œé‡æ–°è®¡ç®—çš„ advantages å’ŒåŸå§‹ advantages å®Œå…¨ç›¸åŒã€‚

### Q4: compute_new_adv å’ŒåŸå§‹ advantages æœ‰å¤šå¤§å·®å¼‚ï¼Ÿ
A: å·®å¼‚å–å†³äºè¿‡æ»¤å¼ºåº¦ã€‚è¿‡æ»¤è¶Šå¼ºï¼ˆkeep_ratio è¶Šå°ï¼‰ï¼Œå·®å¼‚è¶Šå¤§ã€‚å¯ä»¥é€šè¿‡ wandb çš„ `critic/advantages/*` å’Œ `critic/advantages_processed/*` å¯¹æ¯”è§‚å¯Ÿã€‚

### Q5: å‡ºç° "Not enough samples to compute std" è­¦å‘Šæ€ä¹ˆåŠï¼Ÿ
A: è¯´æ˜æŸäº› group ä¿ç•™çš„æ ·æœ¬å¤ªå°‘ï¼ˆ< 2ï¼‰ã€‚å¯ä»¥ï¼š
- å¢åŠ  `keep_pos_ratio` å’Œ `keep_neg_ratio`
- å¢åŠ  `rollout.n`ï¼ˆæ¯ä¸ª prompt ç”Ÿæˆæ›´å¤š responsesï¼‰

## å®Œæ•´é…ç½®ç¤ºä¾‹

```yaml
data:
  train_files: your/dataset
  rollout_batch_size: 128
  max_prompt_length: 2048
  max_response_length: 2048

algorithm:
  adv_estimator: grpo
  gamma: 1.0
  lam: 1.0
  disable_kl: false
  kl_coef: 1.0e-2
  keep_neg_ratio: 0.5      # ä¿ç•™æœ€å·®çš„ 50% è´Ÿæ ·æœ¬
  keep_pos_ratio: 0.7      # ä¿ç•™æœ€å¥½çš„ 70% æ­£æ ·æœ¬
  compute_new_adv: true    # ğŸ†• é‡æ–°è®¡ç®— advantages

worker:
  actor:
    global_batch_size: 64
    micro_batch_size_per_device_for_update: 8
    model:
      model_path: Qwen/Qwen2.5-7B-Instruct
    optim:
      lr: 1.0e-6
  
  rollout:
    n: 16                  # æ¯ä¸ª prompt ç”Ÿæˆ 16 ä¸ª responses
    temperature: 1.0

trainer:
  total_episodes: 10
  logger: ["console", "wandb"]
  project_name: grpo-compute-new-adv
  experiment_name: test
```

## ç‰ˆæœ¬å†å²

- **v1.0** (2025-01): åˆå§‹å®ç°
  - æ”¯æŒåŸºäºä¿ç•™æ ·æœ¬é‡æ–°è®¡ç®— GRPO advantages
  - æ·»åŠ  `compute_new_adv` é…ç½®å‚æ•°
  - æ·»åŠ ç›¸å…³éªŒè¯å’Œè­¦å‘Š

## å‚è€ƒèµ„æ–™

- GRPO è®ºæ–‡: [Group Relative Policy Optimization](https://arxiv.org/abs/2402.03300)
- ç›¸å…³ä»£ç : `verl/trainer/core_algos.py::compute_grpo_outcome_advantage`

